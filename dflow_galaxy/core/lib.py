from dflow.op_template import ScriptOPTemplate
import dflow

from typing import Final, Callable, TypeVar, Optional, Tuple, Union, Annotated
from dataclasses import dataclass
from pathlib import Path
from enum import IntEnum, auto
import os
import shutil
import cloudpickle as cp
import tempfile
import hashlib

T = TypeVar('T')
T_IN_P = TypeVar('T_IN_P')
T_CTX = TypeVar('T_CTX')
T_OUT_P = TypeVar('T_OUT_P')


class Symbol(IntEnum):
    INPUT_PARAMETER = auto()
    INPUT_ARTIFACT = auto()
    OUTPUT_PARAMETER = auto()
    OUTPUT_ARTIFACT = auto()

InputParam = Annotated[T, Symbol.INPUT_PARAMETER]
InputArtifact = Annotated[str, Symbol.INPUT_ARTIFACT]
OutputParam = Annotated[T, Symbol.OUTPUT_PARAMETER]
OutputArtifact = Annotated[str, Symbol.OUTPUT_ARTIFACT]



class DFlowBuilder:
    """
    A type friendly wrapper to build a DFlow workflow.
    """

    def __init__(self, name:str, s3_prefix: str, debug=False):
        """
        :param name: The name of the workflow.
        :param s3_prefix: The base prefix of the S3 bucket to store data generated by the workflow.
        :param debug: If True, the workflow will be run in debug mode.
        """
        if debug:
            dflow.config['mode'] = 'debug'

        self.name: Final[str] = name
        self.s3_prefix: Final[str] = s3_prefix
        self.workflow = dflow.Workflow(name=name)

    def s3_upload(self, path: os.PathLike, *keys: str, debug_func = None) -> str:
        """
        upload local file to S3.

        :param path: The local file path.
        :param keys: The keys of the S3 object.
        :param debug_func: The function to copy the file to the debug directory.
        """
        key = self._s3_get_key(*keys)
        return dflow.upload_s3(path, key, debug_func=debug_func)

    def s3_dump(self, data: Union[bytes, str], *keys: str, debug_func = None) -> str:
        """
        Dump data to s3.

        :param data: The bytes to upload.
        :param keys: The keys of the S3 object.
        :param debug_func: The function to copy the file to the debug directory.
        """
        mode = 'wb' if isinstance(data, bytes) else 'w'
        with tempfile.NamedTemporaryFile(mode) as fp:
            fp.write(data)
            fp.flush()
            return self.s3_upload(Path(fp.name), *keys, debug_func=debug_func)

    def run(self):
        self.workflow.submit()
        self.workflow.wait()

    def add_python_step(self,
                        fn: Callable[[T_IN_P, T_CTX], T_OUT_P],
                        name: Optional[str] = None,
                        with_param=None,
                        mount_path: str = '/tmp/mnt/dflow',
                        ) -> Callable[[T_IN_P, T_CTX], T_OUT_P]:
        """
        Convert a python function to a DFlow step.

        1. Serialize the function and upload to S3.
        2. Build a template to run the function.
        3. Build a step to run the template.
        4. Add the step to the workflow.
        """

        def wrapped_fn(in_params: T_IN_P, ctx: T_CTX):
            # serialize fn and upload to s3
            fn_pickle = cp.dumps(fn, protocol=cp.DEFAULT_PROTOCOL)
            fn_hash = hashlib.sha1(fn_pickle).hexdigest()
            self.s3_dump(fn_pickle, 'system/py-fns', fn_hash, debug_func=shutil.copy)
            fn_path = f'./mnt/system/py-fns/{fn_hash}'

            # generate python code to handle input parameters and artifacts
            argo_in_params_code = ['input_params = {}']
            argo_in_params = {}
            for k, v in in_params.items():
                argo_in_params[k] = dflow.InputParameter(k)
                # The {{arg}} will be replaced by argo workflow with plain text,
                # so we need to use triple quotes to escape the quotes.
                line = '''input_params[%s] = json.loads('["""{{inputs.parameters.%s}}"""]')[0]''' % (repr(k), k)
                argo_in_params_code.append(line)
            return wrapped_fn





    def add_py_step(self,
                    fn: Callable[[T_IN_P, T_IN_A, T_OUT_A], T_OUT_P],
                    name: Optional[str] = None,
                    with_param = None,
                    python_cmd: str = 'python3',
                    ) -> Callable[[T_IN_P, T_IN_A, T_OUT_A], StepResult[T_OUT_P, T_OUT_A]]:
        """
        Add a python step to the workflow.
        """

        def _fn(in_params: T_IN_P, in_artifacts: T_IN_A, out_artifacts: T_OUT_A):
            # serialize fn and upload to s3
            fn_pickle = cp.dumps(fn, protocol=cp.DEFAULT_PROTOCOL)
            fn_hash = hashlib.sha1(fn_pickle).hexdigest()
            with tempfile.NamedTemporaryFile('wb') as fp:
                fp.write(fn_pickle)
                fp.flush()
                fn_s3_key = self.s3_upload(Path(fp.name), 'argo/py-fns', fn_hash, debug_func=shutil.copy)
            fn_path = f'./mnt/argo/py-fns/{fn_hash}'

            # generate python code to handle input parameters and artifacts
            argo_in_params_code = ['input_params = {}']
            argo_in_params = {}
            for k, v in in_params.items():
                argo_in_params[k] = dflow.InputParameter()
                # The {{arg}} will be replaced by argo workflow with plain text,
                # so we need to use triple quotes to escape the quotes.
                line = '''input_params[%s] = json.loads(r'["""{{inputs.parameters.%s}}"""]')[0]''' % (repr(k), k)
                argo_in_params_code.append(line)

            argo_in_artifacts_code = ['input_artifacts = {}']
            argo_in_artifacts = {}
            for k, v in in_artifacts.items():
                mnt_path = os.path.join('./mnt/argo/inputs/artifacts', k)
                argo_in_artifacts[k] = dflow.InputArtifact(path=mnt_path)
                input_artifact_line = f'input_artifacts[{repr(k)}] = {repr(mnt_path)}'
                argo_in_artifacts_code.append(input_artifact_line)
            argo_in_artifacts['__fn__'] = dflow.InputArtifact(path=fn_path)
            argo_in_artifacts_code.append(f'input_artifacts["__fn__"] = {repr(fn_path)}')

            argo_ret_path = './mnt/argo/outputs/parameters/ret.json'
            argo_out_params = {
                'ret': OutputParameter(value_from_path=argo_ret_path),
            }

            argo_out_artifacts_lines = ['output_artifacts = {}']
            argo_out_artifacts = {}
            for k, v in out_artifacts.items():
                mnt_path = os.path.join('./mnt/argo/outputs/artifacts', k)
                argo_out_artifacts[k] = OutputArtifact(path=Path(mnt_path))
                output_artifact_line = f'output_artifacts[{repr(k)}] = {repr(mnt_path)}'
                argo_out_artifacts_lines.append(output_artifact_line)

            template = ScriptOPTemplate(
                name='py-template-' + fn_hash,
                command=python_cmd,
                script='\n'.join([
                    f'fn_path = {repr(fn_path)}',
                    f'argo_ret_path = {repr(argo_ret_path)}',
                    '',
                    'import os, json, cloudpickle as cp',
                    'def ensure_dir(path):',
                    '    os.makedirs(os.path.dirname(path), exist_ok=True)',
                    'with open(fn_path, "rb") as fp:',
                    '    fn = cp.load(fp)',
                    *argo_in_params_code,
                    *argo_in_artifacts_code,
                    *argo_out_artifacts_lines,
                    'ret = fn(input_params, input_artifacts, output_artifacts)',
                    'ensure_dir(argo_ret_path)',
                    'with open(argo_ret_path, "w") as fp:',
                    '    json.dump(ret, fp)',
                ])
            )
            if argo_in_params:
                template.inputs.parameters = argo_in_params
            if argo_in_artifacts:
                template.inputs.artifacts = argo_in_artifacts
            if argo_out_params:
                template.outputs.parameters = argo_out_params
            if argo_out_artifacts:
                template.outputs.artifacts = argo_out_artifacts

            # build step
            step_name = f'py-step-{fn_hash}' if name is None else name
            step = Step(
                name=step_name,
                template=template,
                artifacts={**in_artifacts, '__fn__': dflow.S3Artifact(fn_s3_key)},
                parameters=in_params,
            )

            if with_param is not None:
                step.with_param = with_param

            self.workflow.add(step)
            return StepResult(step)
        return _fn


    def _s3_get_key(self, *keys: str):
        return os.path.join(self.s3_prefix, *keys)
